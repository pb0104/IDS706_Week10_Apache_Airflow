
# ğŸ“Š Customer Orders Data Pipeline (Apache Airflow + PostgreSQL)

This project implements an **end-to-end ETL and visualization pipeline** using **Apache Airflow** to automate ingestion, loading, and analytics of customer order data.
It reads CSV files (`Customers.csv` and `Orders.csv`), loads them into **PostgreSQL**, generates a **visual summary of order totals by gender**, and cleans up temporary files â€” all orchestrated by Airflow.

---

## ğŸ§­ Project Overview

The goal of this pipeline is to demonstrate a **modular and automated data workflow** for analytics teams.

The DAG performs the following operations:

1. **Read input CSVs** â€” `Customers.csv` and `Orders.csv`
2. **Load data into PostgreSQL** (creates schema and tables if not present)
3. **Join tables and analyze data** (aggregate order totals by gender or age group)
4. **Generate visualization** 
5. **Clean up intermediate files** while preserving results

The DAG runs **once on demand** (`@once` schedule), simulating a mini ETL cycle from ingestion to visualization.

---

## âš™ï¸ Architecture

### **High-Level Workflow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1ï¸âƒ£ Read Customers.csv       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2ï¸âƒ£ Read Orders.csv          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3ï¸âƒ£ Load to PostgreSQL       â”‚
â”‚     - Customers             â”‚
â”‚     - Orders                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4ï¸âƒ£ Create Visualization     â”‚
â”‚     (order_total_by_gender) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5ï¸âƒ£ Cleanup Folder           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

![DAG](./screenshots/dag_view.png)




## Tech Stack

| Component          | Purpose                                        |
| ------------------ | ---------------------------------------------- |
| **Apache Airflow** | Workflow orchestration and task scheduling     |
| **PostgreSQL**     | Central database for structured data storage   |
| **Pandas**         | Data manipulation and SQL query handling       |
| **Matplotlib**     | Visualization generation (bar charts)          |
| **Docker Compose** | Local container orchestration for Airflow & DB |
| **Python 3.12**    | Core scripting and DAG definition              |

---


## ğŸ“¦ DAG Design & Tasks

### **DAG ID:** `pipeline`

| Step | Task ID                        | Description                                        | Output                      |
| ---- | ------------------------------ | -------------------------------------------------- | --------------------------- |
| 1ï¸âƒ£  | `read_customers_file`          | Locate and validate `Customers.csv`                | Path to file                |
| 2ï¸âƒ£  | `read_orders_file`             | Locate and validate `Orders.csv`                   | Path to file                |
| 3ï¸âƒ£  | `load_csv_to_pg`               | Load both CSVs into Postgres | Database tables             |
| 4ï¸âƒ£  | `create_visualization_from_db` | Join data, aggregate totals, create bar chart      | `order_total_by_gender.png` |
| 5ï¸âƒ£  | `clear_folder`                 | Clean up non-visualization files                   | Clean workspace             |



## ğŸ§® Data Flow Summary

**Input Files:**

* `/opt/airflow/data/Customers.csv`
* `/opt/airflow/data/Orders.csv`

**Database Tables:**

* `Customers`
* `Orders`

**Visualization Output:**

* `/opt/airflow/data/visualizations/order_total_by_gender.png`

**Logs & Metadata:**

* Stored under `/opt/airflow/logs/`




## ğŸ“ˆ Sample Visualization

The generated bar chart aggregates order totals by gender (if available), or by **age group buckets** if gender data is missing.

![Order Total by Gender](./screenshots/viz_order_total_by_gender.png)



## ğŸš€ Deployment Guide

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/pb0104/customer-orders-pipeline.git

cd customer-orders-pipeline
```

### 2ï¸âƒ£ Start the Airflow Environment

```bash
docker-compose up -d
```

This launches:

* Airflow Webserver â†’ [http://localhost:8080](http://localhost:8080)
* Airflow Scheduler
* PostgreSQL Database

### 3ï¸âƒ£ Add Data Files

Place your input CSV files in:

```bash
/opt/airflow/data/Customers.csv
/opt/airflow/data/Orders.csv
```

*(If file name differs, adjust inside `read_orders_file` or rename accordingly.)*

### 4ï¸âƒ£ Access Airflow UI

Login with default credentials and trigger the DAG manually:

```
DAGs â†’ pipeline â†’ â–¶ï¸ Trigger DAG
```

Alternatively, run from terminal:

```bash
docker exec -it docker-airflow-scheduler-1 airflow dags trigger pipeline
```



## ğŸ§¹ Automatic Cleanup

After visualization creation, the DAG **clears non-essential files** in the `/data` directory, preserving:

* `/data/visualizations/`
* Generated plots

Logs remain intact for auditing and debugging purposes.



##  Key Learnings

* Demonstrates **ETL orchestration** using Airflowâ€™s TaskFlow API
* Handles **data ingestion, transformation, and visualization** seamlessly
* Shows **Postgres integration** via Airflowâ€™s `PostgresHook`
* Incorporates **automated cleanup** for reproducible and maintainable workflows



## ğŸ“‚ Repository Structure

```
IDS706_WEEK10_APACHE_AIRFLOW/
â”‚
â”œâ”€â”€ .devcontainer/               # VSCode container setup
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ db.env
|   â””â”€â”€ docker-compose.yml           #Multi-container Airflow + Postgres setup
|  
â”œâ”€â”€ config/
â”‚   â””â”€â”€ airflow.cfg
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ pipeline.py              # Main DAG file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Customers.csv
â”‚   â”œâ”€â”€ Orders.csv
â”‚   â””â”€â”€ visualizations/
â”‚       â””â”€â”€ order_total_by_gender.png
â”œâ”€â”€ logs/                        # Airflow runtime logs
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ dag_view.png
â”‚   â”œâ”€â”€ dag_run.png
â”‚   â””â”€â”€ viz_order_total_by_gender.png
â””â”€â”€ requirements.txt
```



## ğŸŒŸ Example DAG Run

### Successful DAG Execution

![DAG Run](./screenshots/dag_run.png)

All tasks (read â†’ load â†’ visualize â†’ cleanup) executed successfully in parallelized fashion.



## ğŸ”® Future Enhancements

* Enable **daily scheduled runs** (`@daily`) for continuous ingestion
* Add **data validation checks** using Great Expectations
* Extend visualization to include **order trends by age bucket**
* Integrate a **Streamlit dashboard** to visualize Airflow outputs interactively
